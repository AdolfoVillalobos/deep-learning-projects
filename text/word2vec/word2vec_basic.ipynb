{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word 2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n",
    "\n",
    "\n",
    "Two common methods for learning representations of words:\n",
    "\n",
    "    1. **Continuous Bag-of-Words Model**: Predicts the middle word based on surrounding context words. Context is few words befor and after the current middle word. Note: Order is not important, only context.\n",
    "    2. **Continuous Skip-gram Model**:Predicts words within a certain range before and after the current word in the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Example\n",
    "\n",
    "### Overview of the Skip-gram model\n",
    "\n",
    "In short, a skig-gram predicts the context of a word, given the word. A model is trained on skip-grams. Skip-grams are n-grams that allow tokens to be skipped.\n",
    "\n",
    "The context of a word can be represented through a set of skip-gram pairs of <code> (text_word, context_word)</code>. The context word appears in the neighbooring context of the target word. The context words are given by a window size.\n",
    "\n",
    "As an example, the sentence: <br> The wide road shimmered in the hot sun </br> would produce the following skip-gram pairs for the word <br> shimmered </br> and window size 2:\n",
    "\n",
    "<code> (shimmered, road), (shimmered, wide), (shimmered, in), (shimmered, the)</code>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function\n",
    "\n",
    "**The Objective of Skip-Gram**: maximizing the probability of predicting appropriate context words for a given target word.\n",
    "\n",
    "\n",
    "For a sequence $w_1,...,w_n$, the objective can be written as the average log probability:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\dfrac{1}{T}\\displaystyle\\sum_{t=1}^T\\sum_{-c\\leq j\\leq c, j\\neq 0} \\log p(w_{t+j}| w_t)\n",
    "$$\n",
    "\n",
    "were $c$ is the training context.\n",
    "\n",
    "\n",
    "The basic skip-gram formulation defines this probability using the softmax function:\n",
    "$$\n",
    "p(w_o| w_I) = \\dfrac{\\exp(v_{w_o}'v_{w_I})}{\\displaystyle\\sum_{w=1}^W\\exp(v_w'v_{w_I})}\n",
    "$$\n",
    "\n",
    "were $v$ are the vector representations of the words and $W$ is thesize of the vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Estimating such a loss function can be computationally intractable since the denominator involves multipliying 10 to the power of 5 to 7 operations. Hence, an approximation of the softmax function is performed by using the $NCE$ loss function.\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Further approximation can be done on the NCE loss function since the objective is to learn the word embeddings, and not really the distribution of the words. This is done by using **negative sampling**.\n",
    "\n",
    "**Negative Sampling**: Draw the Context Word from *num_ns* negative samples as in a classification problem. This is done by considering the negative samples to be drawn from a noise distribution $P_W(w)$ of words.\n",
    "\n",
    "\n",
    "A **negative sample** is defined as (target word, context_word) such that the context **does not appear in the window_size neighborhood** of the target_word.\n",
    "\n",
    "\n",
    "As an example in our sentence, negative samples of window size 2 might be.:\n",
    "\n",
    "<code> (hot, shimmered), (wide, hot), (wide, sun) </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Skip-grams for a Single Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize a simple sentence\n",
    "\n",
    "The steps to follow are:\n",
    "1. We tokenize a sentence\n",
    "2. We create a vocabulary dictionary and the inverse mapping.\n",
    "3. Map the sentence using our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun'] 8\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = sentence.lower().split()\n",
    "print(tokens, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n",
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "\n",
    "vocab, index = {}, 1 # We start indexing from 1, since 0 is left for the padding token.\n",
    "vocab[\"<pad>\"] = 0\n",
    "for token in tokens:\n",
    "    if token not in vocab.keys():\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "vocab_size = len(vocab.keys())\n",
    "print(vocab)\n",
    "\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 1, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Map the sentence\n",
    "\n",
    "example_sentence = [vocab[token] for token in tokens]\n",
    "print(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Grams: Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 3], [1, 6], [6, 1], [1, 5], [5, 3], [3, 5], [1, 3], [5, 1], [4, 1], [1, 7], [4, 5], [2, 1], [3, 4], [6, 7], [5, 4], [4, 2], [7, 6], [2, 3], [3, 2], [2, 4], [1, 4], [7, 1], [3, 1], [5, 6], [1, 2], [6, 5]] 26\n",
      "target: shimmered, context: road\n",
      "target: the, context: hot\n",
      "target: hot, context: the\n",
      "target: the, context: in\n",
      "target: in, context: road\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(example_sentence, vocabulary_size=vocab_size,\n",
    "                                                               window_size=window_size, negative_samples=0)\n",
    "print(positive_skip_grams, len(positive_skip_grams))\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"target: {inverse_vocab[target]}, context: {inverse_vocab[context]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated all possible positive skipgram with window size 2. To generate negative samples, we need to sample num_ns from the vocabulary, conditioning that they are not in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n",
      "tf.Tensor([6 4 2 0], shape=(4,), dtype=int64)\n",
      "['hot', 'shimmered', 'wide', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "target_word, context_word = positive_skip_grams[0]\n",
    "print(target_word, context_word)\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1,1))\n",
    "\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "        true_classes=context_class, # Regarded as the positive class\n",
    "        num_true=1, # Each example should be different\n",
    "        num_sampled=num_ns,\n",
    "        unique=True,\n",
    "        range_max=vocab_size,\n",
    "        seed=SEED,\n",
    "        name=\"negative_sampling\")\n",
    "\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Point: num_ns (number of negative samples per positive context word) between [5, 20] is shown to work best for smaller datasets, while num_ns between [2,5] suffices for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the training example.\n",
    "\n",
    "# Add a dimension so you can use concatenation (on the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "\n",
    "# Concat positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "\n",
    "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
    "label = tf.constant([1]+[0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "\n",
    "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label =  tf.squeeze(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 4\n",
      "target_word     : shimmered\n",
      "context_indices : [3 6 4 2 0]\n",
      "context_words   : ['road', 'hot', 'shimmered', 'wide', '<pad>']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training example is the tuple <code> (target_word, context_words, labels)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : tf.Tensor(4, shape=(), dtype=int32)\n",
      "context : tf.Tensor([3 6 4 2 0], shape=(5,), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"target  :\", target)\n",
    "print(f\"context :\", context )\n",
    "print(f\"label   :\", label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unknown file attribute: i\r\n"
     ]
    }
   ],
   "source": [
    "![title](img/word2vec_negative_sampling.png.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
